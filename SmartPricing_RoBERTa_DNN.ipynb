{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0dc40b0",
   "metadata": {},
   "source": [
    "# Smart Pricing — **RoBERTa Embeddings + DNN (100 epochs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f39cbc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs\n",
    "!pip -q install numpy pandas scikit-learn scipy sentence-transformers==3.0.1 transformers==4.44.2 tqdm torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d3c71ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Imports & config\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "DATA_DIR = Path(\"dataset\"); TRAIN_CSV = DATA_DIR/\"train.csv\"; TEST_CSV = DATA_DIR/\"test.csv\"\n",
    "OUTPUT_DIR = Path(\"OUTPUT-3\"); OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED=42; EPOCHS=500; BATCH_SIZE=1024; LR=2e-3; WD=1e-4; DROPOUT=0.2; HIDDEN=[1024,512,256]\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f90de758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sample_id                                    catalog_content  \\\n",
      "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
      "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
      "\n",
      "                                          image_link  price  \n",
      "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n",
      "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "train_df = pd.read_csv(TRAIN_CSV); has_test = TEST_CSV.exists(); test_df = pd.read_csv(TEST_CSV) if has_test else None\n",
    "def normalize_text(s): \n",
    "    if not isinstance(s,str): return \"\"\n",
    "    return re.sub(r\"\\s+\",\" \", s.replace(\"\\n\",\" \").replace(\"\\r\",\" \")).strip()\n",
    "train_df[\"catalog_content\"] = train_df[\"catalog_content\"].astype(str).apply(normalize_text)\n",
    "if has_test: test_df[\"catalog_content\"] = test_df[\"catalog_content\"].astype(str).apply(normalize_text)\n",
    "print(train_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ff94904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying SentenceTransformer: sentence-transformers/paraphrase-roberta-base-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name sentence-transformers/paraphrase-roberta-base-v1. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> failed: OSError: sentence-transformers/paraphrase-roberta-base-v1 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "Trying SentenceTransformer: sentence-transformers/all-distilroberta-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chandu\\OneDrive\\Desktop\\AmazonMLC\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Chandu\\.cache\\huggingface\\hub\\models--sentence-transformers--all-distilroberta-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "c:\\Users\\Chandu\\OneDrive\\Desktop\\AmazonMLC\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Batches: 100%|██████████| 586/586 [1:15:14<00:00,  7.70s/it]   \n",
      "Batches: 100%|██████████| 586/586 [19:04<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shapes: (75000, 768) (75000, 768)\n"
     ]
    }
   ],
   "source": [
    "# 3) RoBERTa sentence embeddings (robust loader with fallbacks)\n",
    "import os, numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\", None)  # optional: set this in your shell if your org requires it\n",
    "\n",
    "def try_sentence_transformer(ids, device):\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    last_err = None\n",
    "    for mid in ids:\n",
    "        try:\n",
    "            print(f\"Trying SentenceTransformer: {mid}\")\n",
    "            return SentenceTransformer(mid, device=device, use_auth_token=HF_TOKEN)\n",
    "        except Exception as e:\n",
    "            print(f\"  -> failed: {e.__class__.__name__}: {e}\")\n",
    "            last_err = e\n",
    "    raise last_err\n",
    "\n",
    "def mean_pool_last_hidden(model_outputs, attention_mask):\n",
    "    # mean pooling excluding padding tokens\n",
    "    token_embeddings = model_outputs.last_hidden_state  # [B, T, H]\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    summed = (token_embeddings * input_mask_expanded).sum(dim=1)\n",
    "    counts = input_mask_expanded.sum(dim=1).clamp(min=1e-9)\n",
    "    return (summed / counts)\n",
    "\n",
    "def build_roberta_fallback(device):\n",
    "    # plain transformers fallback (no sentence-transformers)\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    tok = AutoTokenizer.from_pretrained(\"roberta-base\", use_auth_token=HF_TOKEN)\n",
    "    mdl = AutoModel.from_pretrained(\"roberta-base\", use_auth_token=HF_TOKEN).to(device)\n",
    "    mdl.eval()\n",
    "    def embed(texts, batch_size=64, normalize=True):\n",
    "        embs = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding (roberta-base)\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = tok(batch, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                out = mdl(enc[\"input_ids\"].to(device), attention_mask=enc[\"attention_mask\"].to(device))\n",
    "                pooled = mean_pool_last_hidden(out, enc[\"attention_mask\"].to(device))  # [B, H]\n",
    "                vec = pooled\n",
    "                if normalize:\n",
    "                    vec = torch.nn.functional.normalize(vec, p=2, dim=1)\n",
    "                embs.append(vec.cpu().numpy())\n",
    "        return np.vstack(embs).astype(np.float32)\n",
    "    return embed\n",
    "\n",
    "RO_BERTA_CANDIDATES = [\n",
    "    # public, RoBERTa-based sentence-transformers (no token needed in most setups)\n",
    "    \"sentence-transformers/paraphrase-roberta-base-v1\",\n",
    "    \"sentence-transformers/all-distilroberta-v1\",\n",
    "    # the one you tried (kept last, sometimes requires auth in certain mirrors)\n",
    "    \"sentence-transformers/all-roberta-base-v1\",\n",
    "]\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embed_fn = None\n",
    "try:\n",
    "    # First try sentence-transformers variants\n",
    "    st_model = try_sentence_transformer(RO_BERTA_CANDIDATES, DEVICE)\n",
    "    def embed_fn(texts, batch_size=256, normalize=True):\n",
    "        # sentence_transformers already returns normalized if normalize_embeddings=True\n",
    "        embs = st_model.encode(texts, batch_size=batch_size, show_progress_bar=True,\n",
    "                               normalize_embeddings=True)\n",
    "        return np.asarray(embs, dtype=np.float32)\n",
    "except Exception as e:\n",
    "    print(\"All sentence-transformers RoBERTa attempts failed; falling back to plain roberta-base.\")\n",
    "    embed_fn = build_roberta_fallback(DEVICE)\n",
    "\n",
    "def encode_texts(texts, batch_size=256):\n",
    "    return embed_fn(texts, batch_size=batch_size, normalize=True)\n",
    "\n",
    "# Build embeddings\n",
    "train_texts = train_df[\"catalog_content\"].fillna(\"\").tolist()\n",
    "X_train_emb = encode_texts(train_texts, batch_size=128)  # lower batch if OOM\n",
    "\n",
    "X_test_emb = None\n",
    "if has_test:\n",
    "    test_texts = test_df[\"catalog_content\"].fillna(\"\").tolist()\n",
    "    X_test_emb = encode_texts(test_texts, batch_size=128)\n",
    "\n",
    "print(\"Embedding shapes:\", X_train_emb.shape, None if X_test_emb is None else X_test_emb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32a8c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # RoBERTa embeddings\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# MODEL_NAME = \"sentence-transformers/all-roberta-base-v1\"\n",
    "# st_model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
    "# def encode_texts(texts, batch_size=256):\n",
    "#     embs = st_model.encode(texts, batch_size=batch_size, show_progress_bar=True, normalize_embeddings=True)\n",
    "#     return np.asarray(embs, dtype=np.float32)\n",
    "# X_train_emb = encode_texts(train_df[\"catalog_content\"].fillna(\"\").tolist())\n",
    "# X_test_emb = encode_texts(test_df[\"catalog_content\"].fillna(\"\").tolist()) if has_test else None\n",
    "# print(\"Shapes:\", X_train_emb.shape, None if X_test_emb is None else X_test_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a158fd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Targets\n",
    "y = train_df[\"price\"].astype(float).values\n",
    "y_clip = np.clip(y, np.percentile(y,1), np.percentile(y,99))\n",
    "scaler_y = StandardScaler().fit(np.log1p(y_clip).reshape(-1,1))\n",
    "y_std = scaler_y.transform(np.log1p(y).reshape(-1,1)).ravel()\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X_train_emb, y_std, test_size=0.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edae93f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | train 0.5422 | valid 0.5214\n",
      "Epoch 002 | train 0.4924 | valid 0.4891\n",
      "Epoch 003 | train 0.4702 | valid 0.4724\n",
      "Epoch 004 | train 0.4556 | valid 0.4607\n",
      "Epoch 005 | train 0.4431 | valid 0.4510\n",
      "Epoch 010 | train 0.4009 | valid 0.4300\n",
      "Epoch 020 | train 0.3474 | valid 0.4162\n",
      "Epoch 030 | train 0.2934 | valid 0.4177\n",
      "Epoch 040 | train 0.2414 | valid 0.4198\n",
      "Epoch 050 | train 0.1963 | valid 0.4194\n",
      "Epoch 060 | train 0.1665 | valid 0.4203\n",
      "Epoch 070 | train 0.1448 | valid 0.4351\n",
      "Epoch 080 | train 0.1321 | valid 0.4322\n",
      "Epoch 090 | train 0.1199 | valid 0.4297\n",
      "Epoch 100 | train 0.1121 | valid 0.4404\n",
      "Epoch 110 | train 0.1033 | valid 0.4342\n",
      "Epoch 120 | train 0.1003 | valid 0.4298\n",
      "Epoch 130 | train 0.0932 | valid 0.4327\n",
      "Epoch 140 | train 0.0878 | valid 0.4332\n",
      "Epoch 150 | train 0.0830 | valid 0.4354\n",
      "Epoch 160 | train 0.0775 | valid 0.4309\n",
      "Epoch 170 | train 0.0720 | valid 0.4320\n",
      "Epoch 180 | train 0.0698 | valid 0.4315\n",
      "Epoch 190 | train 0.0666 | valid 0.4369\n",
      "Epoch 200 | train 0.0643 | valid 0.4389\n",
      "Epoch 210 | train 0.0603 | valid 0.4335\n",
      "Epoch 220 | train 0.0576 | valid 0.4335\n",
      "Epoch 230 | train 0.0546 | valid 0.4332\n",
      "Epoch 240 | train 0.0523 | valid 0.4323\n",
      "Epoch 250 | train 0.0510 | valid 0.4298\n",
      "Epoch 260 | train 0.0486 | valid 0.4310\n",
      "Epoch 270 | train 0.0462 | valid 0.4363\n",
      "Epoch 280 | train 0.0440 | valid 0.4295\n",
      "Epoch 290 | train 0.0420 | valid 0.4331\n",
      "Epoch 300 | train 0.0399 | valid 0.4352\n",
      "Epoch 310 | train 0.0384 | valid 0.4304\n",
      "Epoch 320 | train 0.0365 | valid 0.4326\n",
      "Epoch 330 | train 0.0352 | valid 0.4303\n",
      "Epoch 340 | train 0.0333 | valid 0.4340\n",
      "Epoch 350 | train 0.0323 | valid 0.4301\n",
      "Epoch 360 | train 0.0306 | valid 0.4290\n",
      "Epoch 370 | train 0.0298 | valid 0.4325\n",
      "Epoch 380 | train 0.0289 | valid 0.4291\n",
      "Epoch 390 | train 0.0273 | valid 0.4322\n",
      "Epoch 400 | train 0.0265 | valid 0.4289\n",
      "Epoch 410 | train 0.0260 | valid 0.4304\n",
      "Epoch 420 | train 0.0252 | valid 0.4291\n",
      "Epoch 430 | train 0.0242 | valid 0.4290\n",
      "Epoch 440 | train 0.0238 | valid 0.4287\n",
      "Epoch 450 | train 0.0236 | valid 0.4309\n",
      "Epoch 460 | train 0.0231 | valid 0.4293\n",
      "Epoch 470 | train 0.0229 | valid 0.4286\n",
      "Epoch 480 | train 0.0227 | valid 0.4287\n",
      "Epoch 490 | train 0.0227 | valid 0.4295\n",
      "Epoch 500 | train 0.0226 | valid 0.4294\n"
     ]
    }
   ],
   "source": [
    "# DNN\n",
    "class TabDS(Dataset):\n",
    "    def __init__(self, X, y=None): self.X=torch.from_numpy(X).float(); self.y=None if y is None else torch.from_numpy(y).float()\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self,i): return (self.X[i], self.y[i]) if self.y is not None else self.X[i]\n",
    "\n",
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, in_dim, hidden=[1024,512,256], dropout=0.2):\n",
    "#         super().__init__(); layers=[]; d=in_dim\n",
    "#         for h in hidden: layers += [nn.Linear(d,h), nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(dropout)]; d=h\n",
    "#         layers += [nn.Linear(d,1)]; self.net=nn.Sequential(*layers)\n",
    "#     def forward(self,x): return self.net(x).squeeze(-1)\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=[768, 512, 256], dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.in_norm = nn.LayerNorm(in_dim)  # stabilize input scale\n",
    "        layers, d = [], in_dim\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(d, h), nn.GELU(), nn.Dropout(dropout)]\n",
    "            d = h\n",
    "        layers += [nn.Linear(d, 1)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.net(self.in_norm(x)).squeeze(-1)\n",
    "\n",
    "\n",
    "def train_model(X_tr, y_tr, X_va, y_va, in_dim):\n",
    "    model = MLP(in_dim).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "    loss_fn = nn.SmoothL1Loss(beta=0.5)  # Huber\n",
    "    tr_dl = DataLoader(TabDS(X_tr, y_tr), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    va_dl = DataLoader(TabDS(X_va, y_va), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        opt, max_lr=2e-3, epochs=EPOCHS, steps_per_epoch=len(tr_dl)\n",
    "    )\n",
    "    CLIP_NORM = 1.0\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train(); tr_loss = 0.0\n",
    "        for xb, yb in tr_dl:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "            tr_loss += loss.item() * xb.size(0)\n",
    "        tr_loss /= len(tr_dl.dataset)\n",
    "\n",
    "        # validation\n",
    "        model.eval(); va_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in va_dl:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                pred = model(xb)\n",
    "                va_loss += loss_fn(pred, yb).item() * xb.size(0)\n",
    "        va_loss /= len(va_dl.dataset)\n",
    "        if ep % 10 == 0 or ep <= 5:\n",
    "            print(f\"Epoch {ep:03d} | train {tr_loss:.4f} | valid {va_loss:.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train_model(X_tr,y_tr,X_va,y_va,X_train_emb.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c80c7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FULL] Epoch 001 | loss 0.8150\n",
      "[FULL] Epoch 002 | loss 0.6799\n",
      "[FULL] Epoch 003 | loss 0.6301\n",
      "[FULL] Epoch 004 | loss 0.5868\n",
      "[FULL] Epoch 005 | loss 0.5505\n",
      "[FULL] Epoch 010 | loss 0.3901\n",
      "[FULL] Epoch 020 | loss 0.2247\n",
      "[FULL] Epoch 030 | loss 0.1686\n",
      "[FULL] Epoch 040 | loss 0.1424\n",
      "[FULL] Epoch 050 | loss 0.1275\n",
      "[FULL] Epoch 060 | loss 0.1165\n",
      "[FULL] Epoch 070 | loss 0.1085\n",
      "[FULL] Epoch 080 | loss 0.1009\n",
      "[FULL] Epoch 090 | loss 0.0938\n",
      "[FULL] Epoch 100 | loss 0.0914\n",
      "[FULL] Epoch 110 | loss 0.0880\n",
      "[FULL] Epoch 120 | loss 0.0823\n",
      "[FULL] Epoch 130 | loss 0.0819\n",
      "[FULL] Epoch 140 | loss 0.0787\n",
      "[FULL] Epoch 150 | loss 0.0765\n",
      "[FULL] Epoch 160 | loss 0.0736\n",
      "[FULL] Epoch 170 | loss 0.0704\n",
      "[FULL] Epoch 180 | loss 0.0700\n",
      "[FULL] Epoch 190 | loss 0.0673\n",
      "[FULL] Epoch 200 | loss 0.0655\n",
      "[FULL] Epoch 210 | loss 0.0645\n",
      "[FULL] Epoch 220 | loss 0.0647\n",
      "[FULL] Epoch 230 | loss 0.0610\n",
      "[FULL] Epoch 240 | loss 0.0623\n",
      "[FULL] Epoch 250 | loss 0.0587\n",
      "[FULL] Epoch 260 | loss 0.0585\n",
      "[FULL] Epoch 270 | loss 0.0578\n",
      "[FULL] Epoch 280 | loss 0.0564\n",
      "[FULL] Epoch 290 | loss 0.0561\n",
      "[FULL] Epoch 300 | loss 0.0544\n",
      "[FULL] Epoch 310 | loss 0.0543\n",
      "[FULL] Epoch 320 | loss 0.0551\n",
      "[FULL] Epoch 330 | loss 0.0524\n",
      "[FULL] Epoch 340 | loss 0.0524\n",
      "[FULL] Epoch 350 | loss 0.0510\n",
      "[FULL] Epoch 360 | loss 0.0496\n",
      "[FULL] Epoch 370 | loss 0.0500\n",
      "[FULL] Epoch 380 | loss 0.0492\n",
      "[FULL] Epoch 390 | loss 0.0485\n",
      "[FULL] Epoch 400 | loss 0.0491\n",
      "[FULL] Epoch 410 | loss 0.0487\n",
      "[FULL] Epoch 420 | loss 0.0472\n",
      "[FULL] Epoch 430 | loss 0.0480\n",
      "[FULL] Epoch 440 | loss 0.0468\n",
      "[FULL] Epoch 450 | loss 0.0462\n",
      "[FULL] Epoch 460 | loss 0.0465\n",
      "[FULL] Epoch 470 | loss 0.0459\n",
      "[FULL] Epoch 480 | loss 0.0473\n",
      "[FULL] Epoch 490 | loss 0.0446\n",
      "[FULL] Epoch 500 | loss 0.0444\n"
     ]
    }
   ],
   "source": [
    "# Retrain on full set for 100 epochs\n",
    "model_full = MLP(X_train_emb.shape[1], HIDDEN, DROPOUT).to(DEVICE)\n",
    "opt=torch.optim.AdamW(model_full.parameters(), lr=LR, weight_decay=WD); loss_fn=nn.MSELoss()\n",
    "dl=DataLoader(TabDS(X_train_emb, y_std), batch_size=BATCH_SIZE, shuffle=True)\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model_full.train(); loss_sum=0.0\n",
    "    for xb,yb in dl:\n",
    "        xb,yb=xb.to(DEVICE), yb.to(DEVICE); opt.zero_grad(); pred=model_full(xb); loss=loss_fn(pred,yb); loss.backward(); opt.step(); loss_sum+=loss.item()*xb.size(0)\n",
    "    loss_sum/=len(dl.dataset)\n",
    "    if ep%10==0 or ep<=5: print(f\"[FULL] Epoch {ep:03d} | loss {loss_sum:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c660b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation SMAPE: 56.311%\n"
     ]
    }
   ],
   "source": [
    "# --- Compute SMAPE on validation set ---\n",
    "def smape(y_true, y_pred, eps=1e-9):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    diff = np.abs(y_pred - y_true) / denom\n",
    "    return 100.0 * np.mean(diff)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    xva = torch.from_numpy(X_va).float().to(DEVICE)\n",
    "    pred_std = model(xva).cpu().numpy()\n",
    "\n",
    "# inverse standardization + log transform\n",
    "log_pred = scaler_y.inverse_transform(pred_std.reshape(-1,1)).ravel()\n",
    "log_true = scaler_y.inverse_transform(y_va.reshape(-1,1)).ravel()\n",
    "\n",
    "# back to price scale\n",
    "y_pred_price = np.expm1(log_pred)\n",
    "y_true_price = np.expm1(log_true)\n",
    "\n",
    "val_smape = smape(y_true_price, y_pred_price)\n",
    "print(f\"Validation SMAPE: {val_smape:.3f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6a6a3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: OUTPUT-3\\submission_roberta_dnn.csv\n"
     ]
    }
   ],
   "source": [
    "# Predict & save submission\n",
    "if (DATA_DIR/\"test.csv\").exists():\n",
    "    model_full.eval()\n",
    "    with torch.no_grad():\n",
    "        xt=torch.from_numpy(X_test_emb).float().to(DEVICE); pred_std=model_full(xt).cpu().numpy()\n",
    "    log_price = scaler_y.inverse_transform(pred_std.reshape(-1,1)).ravel()\n",
    "    price = np.expm1(log_price)\n",
    "    sub = pd.DataFrame({\"sample_id\": pd.read_csv(DATA_DIR/'test.csv')[\"sample_id\"], \"price\": price})\n",
    "    outp = OUTPUT_DIR/\"submission_roberta_dnn.csv\"; sub.to_csv(outp, index=False); print(\"Saved:\", outp)\n",
    "else:\n",
    "    print(\"No test.csv found; skip submission.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a4f16f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Fold 1 ==========\n",
      "[Fold train] epoch 001 | train 0.5485 | valid 0.5173\n",
      "[Fold train] epoch 002 | train 0.4978 | valid 0.4841\n",
      "[Fold train] epoch 003 | train 0.4714 | valid 0.4631\n",
      "[Fold train] epoch 004 | train 0.4537 | valid 0.4526\n",
      "[Fold train] epoch 005 | train 0.4373 | valid 0.4477\n",
      "[Fold train] epoch 010 | train 0.3750 | valid 0.4286\n",
      "[Fold train] epoch 020 | train 0.2645 | valid 0.4300\n",
      "[Fold train] epoch 030 | train 0.1888 | valid 0.4339\n",
      "[Fold train] epoch 040 | train 0.1314 | valid 0.4380\n",
      "[Fold train] epoch 050 | train 0.1000 | valid 0.4345\n",
      "[Fold train] epoch 060 | train 0.0797 | valid 0.4333\n",
      "[Fold train] epoch 070 | train 0.0620 | valid 0.4329\n",
      "[Fold train] epoch 080 | train 0.0521 | valid 0.4290\n",
      "[Fold train] epoch 090 | train 0.0458 | valid 0.4287\n",
      "[Fold train] epoch 100 | train 0.0440 | valid 0.4289\n",
      "\n",
      "========== Fold 2 ==========\n",
      "[Fold train] epoch 001 | train 0.5460 | valid 0.5181\n",
      "[Fold train] epoch 002 | train 0.4973 | valid 0.4883\n",
      "[Fold train] epoch 003 | train 0.4740 | valid 0.4693\n",
      "[Fold train] epoch 004 | train 0.4563 | valid 0.4548\n",
      "[Fold train] epoch 005 | train 0.4395 | valid 0.4475\n",
      "[Fold train] epoch 010 | train 0.3747 | valid 0.4241\n",
      "[Fold train] epoch 020 | train 0.2699 | valid 0.4245\n",
      "[Fold train] epoch 030 | train 0.1921 | valid 0.4270\n",
      "[Fold train] epoch 040 | train 0.1350 | valid 0.4289\n",
      "[Fold train] epoch 050 | train 0.1018 | valid 0.4207\n",
      "[Fold train] epoch 060 | train 0.0796 | valid 0.4276\n",
      "[Fold train] epoch 070 | train 0.0632 | valid 0.4281\n",
      "[Fold train] epoch 080 | train 0.0522 | valid 0.4258\n",
      "[Fold train] epoch 090 | train 0.0460 | valid 0.4227\n",
      "[Fold train] epoch 100 | train 0.0449 | valid 0.4235\n",
      "\n",
      "========== Fold 3 ==========\n",
      "[Fold train] epoch 001 | train 0.5474 | valid 0.5169\n",
      "[Fold train] epoch 002 | train 0.4974 | valid 0.4833\n",
      "[Fold train] epoch 003 | train 0.4722 | valid 0.4723\n",
      "[Fold train] epoch 004 | train 0.4578 | valid 0.4576\n",
      "[Fold train] epoch 005 | train 0.4389 | valid 0.4461\n",
      "[Fold train] epoch 010 | train 0.3732 | valid 0.4216\n",
      "[Fold train] epoch 020 | train 0.2642 | valid 0.4287\n",
      "[Fold train] epoch 030 | train 0.1896 | valid 0.4412\n",
      "[Fold train] epoch 040 | train 0.1333 | valid 0.4339\n",
      "[Fold train] epoch 050 | train 0.1014 | valid 0.4375\n",
      "[Fold train] epoch 060 | train 0.0783 | valid 0.4327\n",
      "[Fold train] epoch 070 | train 0.0627 | valid 0.4315\n",
      "[Fold train] epoch 080 | train 0.0517 | valid 0.4307\n",
      "[Fold train] epoch 090 | train 0.0452 | valid 0.4288\n",
      "[Fold train] epoch 100 | train 0.0432 | valid 0.4286\n",
      "\n",
      "========== Fold 4 ==========\n",
      "[Fold train] epoch 001 | train 0.5495 | valid 0.5126\n",
      "[Fold train] epoch 002 | train 0.4987 | valid 0.4810\n",
      "[Fold train] epoch 003 | train 0.4740 | valid 0.4631\n",
      "[Fold train] epoch 004 | train 0.4550 | valid 0.4496\n",
      "[Fold train] epoch 005 | train 0.4398 | valid 0.4374\n",
      "[Fold train] epoch 010 | train 0.3741 | valid 0.4231\n",
      "[Fold train] epoch 020 | train 0.2692 | valid 0.4233\n",
      "[Fold train] epoch 030 | train 0.1899 | valid 0.4360\n",
      "[Fold train] epoch 040 | train 0.1351 | valid 0.4379\n",
      "[Fold train] epoch 050 | train 0.1006 | valid 0.4343\n",
      "[Fold train] epoch 060 | train 0.0793 | valid 0.4305\n",
      "[Fold train] epoch 070 | train 0.0632 | valid 0.4306\n",
      "[Fold train] epoch 080 | train 0.0518 | valid 0.4281\n",
      "[Fold train] epoch 090 | train 0.0467 | valid 0.4257\n",
      "[Fold train] epoch 100 | train 0.0446 | valid 0.4263\n",
      "\n",
      "========== Fold 5 ==========\n",
      "[Fold train] epoch 001 | train 0.5511 | valid 0.5121\n",
      "[Fold train] epoch 002 | train 0.4994 | valid 0.4788\n",
      "[Fold train] epoch 003 | train 0.4731 | valid 0.4649\n",
      "[Fold train] epoch 004 | train 0.4558 | valid 0.4517\n",
      "[Fold train] epoch 005 | train 0.4400 | valid 0.4418\n",
      "[Fold train] epoch 010 | train 0.3738 | valid 0.4277\n",
      "[Fold train] epoch 020 | train 0.2677 | valid 0.4327\n",
      "[Fold train] epoch 030 | train 0.1898 | valid 0.4391\n",
      "[Fold train] epoch 040 | train 0.1344 | valid 0.4372\n",
      "[Fold train] epoch 050 | train 0.1026 | valid 0.4375\n",
      "[Fold train] epoch 060 | train 0.0787 | valid 0.4367\n",
      "[Fold train] epoch 070 | train 0.0637 | valid 0.4332\n",
      "[Fold train] epoch 080 | train 0.0523 | valid 0.4290\n",
      "[Fold train] epoch 090 | train 0.0463 | valid 0.4299\n",
      "[Fold train] epoch 100 | train 0.0445 | valid 0.4301\n",
      "\n",
      "5-fold CV SMAPE: 56.868%\n",
      "Saved submission: outputs\\submission_roberta_dnn_cv.csv\n"
     ]
    }
   ],
   "source": [
    "# ====== Helpers ======\n",
    "import numpy as np, torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def smape(y_true, y_pred, eps=1e-9):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    diff = np.abs(y_pred - y_true) / denom\n",
    "    return 100.0 * np.mean(diff)\n",
    "\n",
    "def make_strat_labels(y, max_bins=20, n_splits=5):\n",
    "    import pandas as pd, numpy as np\n",
    "    y = np.asarray(y, float)\n",
    "    bins = pd.qcut(np.log1p(y), q=max_bins, duplicates=\"drop\")\n",
    "    return pd.Series(bins).cat.codes.to_numpy()\n",
    "\n",
    "def train_one_fold(X_tr, y_tr, X_va, y_va, in_dim):\n",
    "    model = MLP(in_dim, hidden=[768,512,256], dropout=DROPOUT).to(DEVICE)  # mild tweak\n",
    "    loss_fn = nn.SmoothL1Loss(beta=0.5)  # Huber\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "    tr_dl = DataLoader(TabDS(X_tr, y_tr), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    va_dl = DataLoader(TabDS(X_va, y_va), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        opt, max_lr=LR, epochs=EPOCHS, steps_per_epoch=len(tr_dl)\n",
    "    )\n",
    "    CLIP_NORM = 1.0\n",
    "\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train(); tr_loss = 0.0\n",
    "        for xb, yb in tr_dl:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "            tr_loss += loss.item() * xb.size(0)\n",
    "        tr_loss /= len(tr_dl.dataset)\n",
    "\n",
    "        # (optional) quick valid print every 10 epochs\n",
    "        if ep % 10 == 0 or ep <= 5:\n",
    "            model.eval(); va_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in va_dl:\n",
    "                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                    va_loss += loss_fn(model(xb), yb).item() * xb.size(0)\n",
    "            va_loss /= len(va_dl.dataset)\n",
    "            print(f\"[Fold train] epoch {ep:03d} | train {tr_loss:.4f} | valid {va_loss:.4f}\")\n",
    "    return model\n",
    "\n",
    "# ====== 5-fold CV training ======\n",
    "y_strat = make_strat_labels(y, n_splits=5)\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof_std = np.zeros(len(train_df), dtype=np.float32)\n",
    "test_std_folds = []\n",
    "models = []\n",
    "\n",
    "for fold, (tr, va) in enumerate(skf.split(X_train_emb, y_strat), 1):\n",
    "    print(f\"\\n========== Fold {fold} ==========\")\n",
    "    model = train_one_fold(\n",
    "        X_train_emb[tr], y_std[tr],\n",
    "        X_train_emb[va], y_std[va],\n",
    "        in_dim=X_train_emb.shape[1]\n",
    "    )\n",
    "    models.append(model)\n",
    "\n",
    "    # OOF predictions (standardized log scale)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        xva = torch.from_numpy(X_train_emb[va]).float().to(DEVICE)\n",
    "        oof_std[va] = model(xva).cpu().numpy()\n",
    "\n",
    "        if 'X_test_emb' in globals() and X_test_emb is not None:\n",
    "            xt = torch.from_numpy(X_test_emb).float().to(DEVICE)\n",
    "            test_std_folds.append(model(xt).cpu().numpy())\n",
    "\n",
    "# ====== CV SMAPE on price scale ======\n",
    "oof_log = scaler_y.inverse_transform(oof_std.reshape(-1,1)).ravel()\n",
    "oof_price = np.expm1(oof_log)\n",
    "cv_smape = smape(train_df[\"price\"].values, oof_price)\n",
    "print(f\"\\n5-fold CV SMAPE: {cv_smape:.3f}%\")\n",
    "\n",
    "# ====== Test preds & submission (if test available) ======\n",
    "if 'X_test_emb' in globals() and X_test_emb is not None:\n",
    "    pred_std = np.mean(test_std_folds, axis=0)  # average on standardized log scale\n",
    "    log_price = scaler_y.inverse_transform(pred_std.reshape(-1,1)).ravel()\n",
    "    price = np.expm1(log_price)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        \"sample_id\": test_df[\"sample_id\"].values,\n",
    "        \"price\": price\n",
    "    })\n",
    "\n",
    "    OUTPUT_DIR = Path(\"outputs\"); OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    outp = OUTPUT_DIR / \"submission_roberta_dnn_cv.csv\"\n",
    "    submission.to_csv(outp, index=False)\n",
    "    print(\"Saved submission:\", outp)\n",
    "else:\n",
    "    print(\"No test embeddings found; skipped submission.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a47b76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d9e6a9",
   "metadata": {},
   "source": [
    "\n",
    "# Smart Product Pricing ‚Äî **Enhanced Text-Only+Stacking** Notebook ‚úÖ\n",
    "\n",
    "This notebook upgrades your baseline to target **SMAPE ‚â§ 47** by implementing:\n",
    "- **Per-unit target** training (then rescale to price).\n",
    "- A parallel **TF‚ÄëIDF ‚Üí Ridge** branch and **blend** with your MiniLM+numeric model.\n",
    "- **Brand target encoding (fold-wise)**, **keyword flags**, and improved **quantity parsing** (ranges, counts, bulk cues).\n",
    "- **Stratified CV on log(per-unit)** bins.\n",
    "- Tuned **XGBoost** params for the MiniLM+numeric branch.\n",
    "- Optional **MiniLM KMeans clusters** as weak categories (one-hot bucketization).\n",
    "\n",
    "> Paths are configurable; the code expects `dataset/train.csv` (and optional `dataset/test.csv` with the same columns but without `price`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4502ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# 0) Environment & Installs\n",
    "# =========================\n",
    "# (Run only once per environment)\n",
    "\n",
    "import sys\n",
    "# Lightweight core libs\n",
    "!pip -q install numpy pandas scikit-learn scipy xgboost==2.1.1\n",
    "\n",
    "# Text models & vectorizers\n",
    "# Adjust CUDA wheel as needed (or remove extra-index-url for CPU-only)\n",
    "!pip -q install sentence-transformers==3.0.1 transformers==4.44.2 torch --extra-index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Optional progress bars\n",
    "!pip -q install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================\n",
    "# 1) Imports, Config & Utilities\n",
    "# =============================\n",
    "import os, re, math, gc, json, warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------- Paths ----------\n",
    "DATA_DIR = Path(\"dataset\")\n",
    "TRAIN_CSV = DATA_DIR / \"train.csv\"   # columns: sample_id, catalog_content, image_link, price\n",
    "TEST_CSV  = DATA_DIR / \"test.csv\"    # optional: same columns except price\n",
    "\n",
    "OUTPUT_DIR = Path(\"outputs\"); OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5\n",
    "\n",
    "def smape(y_true, y_pred, eps=1e-9):\n",
    "    # Symmetric MAPE (in %)\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    diff = np.abs(y_pred - y_true) / denom\n",
    "    return 100.0 * np.mean(diff)\n",
    "\n",
    "print(\"Environment ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084ad0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================\n",
    "# 2) Load the data üì•\n",
    "# ==================\n",
    "assert TRAIN_CSV.exists(), f\"Missing {TRAIN_CSV}\"\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "has_test = TEST_CSV.exists()\n",
    "test_df  = pd.read_csv(TEST_CSV) if has_test else None\n",
    "\n",
    "print(train_df.head(3))\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "if has_test:\n",
    "    print(\"Test shape:\", test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2664a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================\n",
    "# 3) Basic text normalization & brand extraction\n",
    "# =============================================\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = s.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def extract_brand(text: str) -> str:\n",
    "    if not isinstance(text, str): return \"~na~\"\n",
    "    t = text.lower()\n",
    "    m = re.search(r\"item name:\\s*(.+)$\", t, flags=re.IGNORECASE|re.MULTILINE)\n",
    "    cand = text if m is None else m.group(1)\n",
    "    cand = cand.split(\" by \")[0].split(\",\")[0]\n",
    "    cand = re.sub(r\"[^a-zA-Z0-9&+\\-\\s]\", \" \", cand).strip()\n",
    "    cand = re.sub(r\"\\s+\", \" \", cand)\n",
    "    return cand[:50] if cand else \"~na~\"\n",
    "\n",
    "for df in [train_df] + ([test_df] if has_test else []):\n",
    "    df[\"catalog_content\"] = df[\"catalog_content\"].astype(str).apply(normalize_text)\n",
    "    df[\"brand\"] = df[\"catalog_content\"].apply(extract_brand)\n",
    "    \n",
    "train_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea913bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================\n",
    "# 4) Quantity parsing ‚Üí total_units üß™\n",
    "# ===================================\n",
    "VOL_MAP = {\"ml\": 1.0, \"l\": 1000.0, \"fl oz\": 29.5735, \"oz\": 29.5735}\n",
    "WT_MAP  = {\"g\": 1.0, \"kg\": 1000.0, \"lb\": 453.592, \"ounce\": 28.3495}\n",
    "COUNT_TOKENS = {\"count\",\"ct\",\"pcs\",\"pieces\",\"tabs\",\"caps\",\"pack\"}\n",
    "\n",
    "_re_num     = r\"(\\d+(?:[\\.,]\\d+)?)\"\n",
    "_re_range   = rf\"{_re_num}\\s*[-‚Äì]\\s*{_re_num}\"\n",
    "_re_unit    = r\"(ml|l|fl\\s*oz|ounce|oz|g|kg|lb|ct|count|pcs|pieces|tabs|caps)\"\n",
    "_re_pack    = rf\"(?:(?:pack)(?:\\s*of)?\\s*{_re_num})|(({_re_num}))\\s*pack\"\n",
    "\n",
    "def _to_float(s):\n",
    "    try: return float(str(s).replace(\",\", \".\"))\n",
    "    except: return np.nan\n",
    "\n",
    "def unit_to_base(val: float, unit: str):\n",
    "    u = unit.lower().strip()\n",
    "    if u in (\"ml\", \"l\", \"fl oz\", \"oz\"):\n",
    "        base = val if u==\"ml\" else 1000.0*val if u==\"l\" else 29.5735*val  # treat oz as fl oz\n",
    "        return base, \"volume_ml\"\n",
    "    if u in (\"g\", \"kg\", \"lb\", \"ounce\"):\n",
    "        base = val if u==\"g\" else 1000.0*val if u==\"kg\" else 453.592*val if u==\"lb\" else 28.3495*val\n",
    "        return base, \"weight_g\"\n",
    "    if u in (\"ct\",\"count\",\"pcs\",\"pieces\",\"tabs\",\"caps\",\"pack\"):\n",
    "        return val, \"count\"\n",
    "    return np.nan, \"~none~\"\n",
    "\n",
    "def parse_total_units(text: str):\n",
    "    if not isinstance(text, str): return np.nan, \"~none~\"\n",
    "    t = text.lower()\n",
    "\n",
    "    # pack count\n",
    "    pack_count = np.nan\n",
    "    pm = re.search(_re_pack, t)\n",
    "    if pm:\n",
    "        nums = [n for n in pm.groups() if n is not None]\n",
    "        if nums:\n",
    "            pack_count = _to_float(nums[-1])\n",
    "\n",
    "    # range like \"10-12 oz\"\n",
    "    m = re.search(rf\"{_re_range}\\s*{_re_unit}\", t)\n",
    "    if m:\n",
    "        a = _to_float(m.group(1)); b = _to_float(m.group(2)); u = m.group(3).replace(\"  \", \" \").strip()\n",
    "        val = np.mean([a,b])\n",
    "        amt, unit_type = unit_to_base(val, u)\n",
    "        if not np.isnan(amt):\n",
    "            if not np.isnan(pack_count): amt *= pack_count\n",
    "            return amt, unit_type\n",
    "\n",
    "    # single \"12 fl oz\"\n",
    "    m = re.search(rf\"{_re_num}\\s*{_re_unit}\", t)\n",
    "    if m:\n",
    "        v = _to_float(m.group(1)); u = m.group(2).replace(\"  \", \" \").strip()\n",
    "        amt, unit_type = unit_to_base(v, u)\n",
    "        if not np.isnan(amt):\n",
    "            if not np.isnan(pack_count): amt *= pack_count\n",
    "            return amt, unit_type\n",
    "\n",
    "    # count-only fallbacks\n",
    "    for tok in COUNT_TOKENS:\n",
    "        m = re.search(rf\"{_re_num}\\s*{tok}\\b\", t)\n",
    "        if m:\n",
    "            cnt = _to_float(m.group(1))\n",
    "            if not np.isnan(cnt):\n",
    "                if not np.isnan(pack_count): cnt *= pack_count\n",
    "                return cnt, \"count\"\n",
    "\n",
    "    if not np.isnan(pack_count):\n",
    "        return pack_count, \"count\"\n",
    "\n",
    "    return np.nan, \"~none~\"\n",
    "\n",
    "for df in [train_df] + ([test_df] if has_test else []):\n",
    "    parsed = df[\"catalog_content\"].apply(parse_total_units)\n",
    "    df[\"total_units_base\"] = parsed.apply(lambda x: x[0])\n",
    "    df[\"unit_kind\"] = parsed.apply(lambda x: x[1])\n",
    "    flags = {\n",
    "        \"is_value_pack\": df[\"catalog_content\"].str.contains(r\"\\b(value pack|bulk|family size)\\b\", case=False, na=False).astype(int),\n",
    "        \"is_refill\":     df[\"catalog_content\"].str.contains(r\"\\brefill\\b\", case=False, na=False).astype(int),\n",
    "        \"is_variety\":    df[\"catalog_content\"].str.contains(r\"\\bvariety\\b\", case=False, na=False).astype(int),\n",
    "        \"has_range\":     df[\"catalog_content\"].str.contains(r\"\\d+\\s*[-‚Äì]\\s*\\d+\", case=False, na=False).astype(int),\n",
    "    }\n",
    "    for k,v in flags.items():\n",
    "        df[k] = v\n",
    "\n",
    "train_df[[\"catalog_content\",\"brand\",\"total_units_base\",\"unit_kind\",\"is_value_pack\",\"is_refill\",\"is_variety\",\"has_range\"]].head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ac73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================\n",
    "# 5) Target: per-unit, then rescale üéØ\n",
    "# ===================================\n",
    "def build_targets(df: pd.DataFrame):\n",
    "    tu = df[\"total_units_base\"].fillna(1.0).clip(lower=1e-6).astype(float)\n",
    "    y_price = df[\"price\"].astype(float)\n",
    "    y_unit = (y_price / tu).astype(float)\n",
    "    return y_price.values, y_unit.values, tu.values\n",
    "\n",
    "y_price = train_df[\"price\"].values.astype(float)\n",
    "y_price, y_unit, tu_train = build_targets(train_df)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "logy = np.log1p(y_unit).reshape(-1,1)\n",
    "scaler_y = StandardScaler().fit(logy)\n",
    "y_std = scaler_y.transform(logy).ravel()\n",
    "\n",
    "bins = pd.qcut(np.log1p(y_unit), q=20, duplicates=\"drop\")\n",
    "print(\"Targets prepared. Example:\", y_price[:3], y_unit[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ccf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================================\n",
    "# 6) Numeric features (incl. brand enc. placeholder) üî¢\n",
    "# ===================================================\n",
    "def prepare_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    num = pd.DataFrame(index=df.index)\n",
    "    num[\"total_units_base\"] = df[\"total_units_base\"].fillna(df[\"total_units_base\"].median())\n",
    "    for k in [\"volume_ml\",\"weight_g\",\"count\",\"~none~\"]:\n",
    "        num[f\"unit_{k}\"] = (df[\"unit_kind\"]==k).astype(int)\n",
    "    for k in [\"is_value_pack\",\"is_refill\",\"is_variety\",\"has_range\"]:\n",
    "        num[k] = df[k].astype(int)\n",
    "    s = df[\"catalog_content\"].fillna(\"\")\n",
    "    num[\"len_chars\"] = s.str.len().clip(0, 2000)\n",
    "    num[\"len_words\"] = s.apply(lambda x: len(x.split())).clip(0, 400)\n",
    "    num[\"num_digits\"] = s.str.count(r\"\\d\").clip(0, 100)\n",
    "    return num\n",
    "\n",
    "X_num_train = prepare_numeric(train_df)\n",
    "X_num_test  = prepare_numeric(test_df) if has_test else None\n",
    "X_num_train.head(3), X_num_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f76c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================\n",
    "# 7) Brand target encoding (fold-wise) üè∑Ô∏è\n",
    "# =====================================\n",
    "brands = train_df[\"brand\"].fillna(\"~na~\").astype(str).values\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "brand_te = np.zeros(len(train_df), dtype=np.float32)\n",
    "\n",
    "for tr_idx, va_idx in skf.split(np.zeros(len(bins)), bins):\n",
    "    mean_tr = y_unit[tr_idx].mean()\n",
    "    gmean = pd.Series(y_unit[tr_idx]).groupby(pd.Series(brands[tr_idx])).mean()\n",
    "    enc = pd.Series(brands[va_idx]).map(gmean).fillna(mean_tr).values\n",
    "    brand_te[va_idx] = enc\n",
    "\n",
    "X_num_train[\"brand_te_per_unit\"] = brand_te\n",
    "\n",
    "topN = 500\n",
    "vc = pd.Series(brands).value_counts().head(topN).index.tolist()\n",
    "for b in vc:\n",
    "    X_num_train[f\"brand_{b}\"] = (pd.Series(brands)==b).astype(int).values\n",
    "\n",
    "if has_test:\n",
    "    brands_test = test_df[\"brand\"].fillna(\"~na~\").astype(str)\n",
    "    full_gmean = pd.Series(y_unit).groupby(pd.Series(brands)).mean()\n",
    "    test_te = brands_test.map(full_gmean).fillna(y_unit.mean()).values\n",
    "    X_num_test[\"brand_te_per_unit\"] = test_te\n",
    "    for b in vc:\n",
    "        X_num_test[f\"brand_{b}\"] = (brands_test==b).astype(int).values\n",
    "\n",
    "print(\"Numeric features with brand TE:\", X_num_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b018ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==================================\n",
    "# 8) MiniLM sentence embeddings üî§üß†\n",
    "# ==================================\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "st_model = SentenceTransformer(MODEL_NAME, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def encode_texts(texts: List[str], batch_size: int = 256):\n",
    "    embs = st_model.encode(texts, batch_size=batch_size, show_progress_bar=True, normalize_embeddings=True)\n",
    "    return np.asarray(embs, dtype=np.float32)\n",
    "\n",
    "train_texts = train_df[\"catalog_content\"].fillna(\"\").tolist()\n",
    "minilm_train = encode_texts(train_texts)\n",
    "\n",
    "if has_test:\n",
    "    test_texts  = test_df[\"catalog_content\"].fillna(\"\").tolist()\n",
    "    minilm_test = encode_texts(test_texts)\n",
    "else:\n",
    "    minilm_test = None\n",
    "\n",
    "print(\"MiniLM shapes:\", minilm_train.shape, None if minilm_test is None else minilm_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6368b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ======================================\n",
    "# 9) Optional: KMeans clusters on MiniLM\n",
    "# ======================================\n",
    "K_CLUST = 100\n",
    "kmeans = MiniBatchKMeans(n_clusters=K_CLUST, random_state=RANDOM_STATE, batch_size=4096)\n",
    "clus_train = kmeans.fit_predict(minilm_train)\n",
    "clus_train_oh = np.eye(K_CLUST, dtype=np.float32)[clus_train]\n",
    "\n",
    "if has_test:\n",
    "    clus_test = kmeans.predict(minilm_test)\n",
    "    clus_test_oh = np.eye(K_CLUST, dtype=np.float32)[clus_test]\n",
    "else:\n",
    "    clus_test_oh = None\n",
    "\n",
    "print(\"Cluster one-hots:\", clus_train_oh.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a875830",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =======================================================\n",
    "# 10) Assemble design matrices for MiniLM + numeric branch\n",
    "# =======================================================\n",
    "def hstack_safe(*arrs):\n",
    "    mats = []\n",
    "    for a in arrs:\n",
    "        if a is None: continue\n",
    "        if isinstance(a, pd.DataFrame) or isinstance(a, pd.Series):\n",
    "            mats.append(a.values)\n",
    "        else:\n",
    "            mats.append(a)\n",
    "    return np.hstack(mats).astype(np.float32)\n",
    "\n",
    "X_mini_num_train = hstack_safe(minilm_train, X_num_train.values, clus_train_oh)\n",
    "X_mini_num_test  = hstack_safe(minilm_test,  X_num_test.values,  clus_test_oh) if has_test else None\n",
    "\n",
    "print(\"MiniLM+num train shape:\", X_mini_num_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737ee034",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================\n",
    "# 11) TF-IDF (word + char) and Ridge OOF branch\n",
    "# =============================================\n",
    "tfw = TfidfVectorizer(ngram_range=(1,2), max_features=150_000, min_df=3)\n",
    "tfc = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), max_features=80_000, min_df=5)\n",
    "\n",
    "X_tfw = tfw.fit_transform(train_df[\"catalog_content\"].fillna(\"\"))\n",
    "X_tfc = tfc.fit_transform(train_df[\"catalog_content\"].fillna(\"\"))\n",
    "X_tfidf = sparse.hstack([X_tfw, X_tfc]).tocsr()\n",
    "\n",
    "oof_tfidf = np.zeros(len(train_df), dtype=np.float32)\n",
    "ridge_models = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "for fold, (tr, va) in enumerate(skf.split(np.zeros(len(bins)), bins), 1):\n",
    "    m = Ridge(alpha=2.0, random_state=RANDOM_STATE)\n",
    "    m.fit(X_tfidf[tr], y_std[tr])  # standardized log per-unit\n",
    "    oof_tfidf[va] = m.predict(X_tfidf[va])\n",
    "    ridge_models.append(m)\n",
    "    print(f\"[Ridge fold {fold}] done.\")\n",
    "\n",
    "if has_test:\n",
    "    Xt_tfw = tfw.transform(test_df[\"catalog_content\"].fillna(\"\"))\n",
    "    Xt_tfc = tfc.transform(test_df[\"catalog_content\"].fillna(\"\"))\n",
    "    Xt_tfidf = sparse.hstack([Xt_tfw, Xt_tfc]).tocsr()\n",
    "else:\n",
    "    Xt_tfidf = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1261f92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================\n",
    "# 12) XGBoost for MiniLM+numeric branch\n",
    "# =====================================\n",
    "params = dict(\n",
    "    objective=\"reg:squarederror\",\n",
    "    tree_method=\"hist\",\n",
    "    max_depth=8,\n",
    "    min_child_weight=4.0,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=1e-2,\n",
    "    reg_lambda=1.0,\n",
    "    learning_rate=0.03,\n",
    "    nthread=-1,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "oof_mini = np.zeros(len(train_df), dtype=np.float32)\n",
    "xgb_models = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "for fold, (tr, va) in enumerate(skf.split(np.zeros(len(bins)), bins), 1):\n",
    "    dtr = xgb.DMatrix(X_mini_num_train[tr], label=y_std[tr])\n",
    "    dva = xgb.DMatrix(X_mini_num_train[va], label=y_std[va])\n",
    "    watch = [(dtr, \"train\"), (dva, \"valid\")]\n",
    "    m = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=dtr,\n",
    "        num_boost_round=4000,\n",
    "        evals=watch,\n",
    "        verbose_eval=200,\n",
    "        early_stopping_rounds=200\n",
    "    )\n",
    "    oof_mini[va] = m.predict(dva, iteration_range=(0, m.best_iteration+1))\n",
    "    xgb_models.append(m)\n",
    "    print(f\"[XGB fold {fold}] best iters:\", m.best_iteration+1)\n",
    "\n",
    "print(\"Branches OOF ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023577b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===================================\n",
    "# 13) Blend, evaluate OOF (train fold)\n",
    "# ===================================\n",
    "w_mini, w_tfidf = 0.6, 0.4  # tune if needed\n",
    "oof_std_blend = w_mini*oof_mini + w_tfidf*oof_tfidf\n",
    "\n",
    "# inverse standardized log(per-unit) ‚Üí per-unit\n",
    "oof_log_per_unit = (scaler_y.inverse_transform(oof_std_blend.reshape(-1,1))).ravel()\n",
    "oof_per_unit     = np.expm1(oof_log_per_unit)\n",
    "\n",
    "# final price = per_unit * total_units\n",
    "oof_price = (oof_per_unit * tu_train).clip(0.01)\n",
    "\n",
    "cv_smape = smape(train_df[\"price\"].values, oof_price)\n",
    "print(f\"CV SMAPE (OOF): {cv_smape:.3f}%\")\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"sample_id\": train_df[\"sample_id\"],\n",
    "    \"price_true\": train_df[\"price\"].values,\n",
    "    \"price_pred\": oof_price,\n",
    "    \"per_unit_pred\": oof_per_unit,\n",
    "}).to_csv(OUTPUT_DIR / \"oof_predictions.csv\", index=False)\n",
    "print(\"Saved:\", OUTPUT_DIR / \"oof_predictions.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d23d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===============================\n",
    "# 14) Inference & submission file\n",
    "# ===============================\n",
    "if has_test:\n",
    "    # TF-IDF preds\n",
    "    preds_tfidf = np.mean([m.predict(Xt_tfidf) for m in ridge_models], axis=0)\n",
    "\n",
    "    # MiniLM+num preds\n",
    "    dtest = xgb.DMatrix(X_mini_num_test)\n",
    "    preds_mini = np.mean([m.predict(dtest, iteration_range=(0, m.best_iteration+1)) for m in xgb_models], axis=0)\n",
    "\n",
    "    # Blend standardized\n",
    "    preds_std_blend = w_mini*preds_mini + w_tfidf*preds_tfidf\n",
    "\n",
    "    # back to price\n",
    "    log_per_unit = scaler_y.inverse_transform(preds_std_blend.reshape(-1,1)).ravel()\n",
    "    per_unit = np.expm1(log_per_unit)\n",
    "\n",
    "    tu_test = test_df[\"total_units_base\"].fillna(1.0).clip(lower=1e-6).astype(float).values\n",
    "    price_pred = (per_unit * tu_test).clip(0.01)\n",
    "\n",
    "    sub = pd.DataFrame({\n",
    "        \"sample_id\": test_df[\"sample_id\"],\n",
    "        \"price\": price_pred\n",
    "    })\n",
    "    sub_path = OUTPUT_DIR / \"submission.csv\"\n",
    "    sub.to_csv(sub_path, index=False)\n",
    "    print(\"Saved submission:\", sub_path)\n",
    "else:\n",
    "    print(\"No test.csv detected; skipped submission.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eb23c7",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & Tips\n",
    "- If GPU is available for `sentence-transformers`, encoding will be much faster (`device='cuda'`).\n",
    "- Tune `w_mini`/`w_tfidf` to minimize SMAPE on OOF (`np.linspace(0.1,0.9,9)` grid is fine).\n",
    "- If your per-unit parsing is still noisy for some categories, try clipping `y_unit` at the 99.5th percentile **when fitting the scaler only** (keep raw for SMAPE).\n",
    "- You can also try LightGBM for the MiniLM+numeric branch‚Äîsometimes improves a bit with high-dimensional sparse inputs.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}

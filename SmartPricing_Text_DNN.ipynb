{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d631c54",
   "metadata": {},
   "source": [
    "# Smart Product Pricing — **Text+MiniLM + DNN (No XGBoost)** ✅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f974ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 0) Installs\n",
    "!pip -q install numpy pandas scikit-learn scipy sentence-transformers==3.0.1 transformers==4.44.2 tqdm torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d55296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Imports & Config\n",
    "import re, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from scipy import sparse\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATA_DIR = Path(\"dataset\")\n",
    "TRAIN_CSV = DATA_DIR / \"train.csv\"\n",
    "TEST_CSV  = DATA_DIR / \"test.csv\"\n",
    "OUTPUT_DIR = Path(\"outputs\"); OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 30\n",
    "PATIENCE = 5\n",
    "LR = 2e-3\n",
    "WD = 1e-4\n",
    "DROPOUT = 0.2\n",
    "HIDDEN = [1024, 512, 256]\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def smape(y_true, y_pred, eps=1e-9):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    denom = (np.abs(y_true) + np.abs(y_pred) + eps) / 2.0\n",
    "    diff = np.abs(y_pred - y_true) / denom\n",
    "    return 100.0 * np.mean(diff)\n",
    "\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e8911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2) Load\n",
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "has_test = TEST_CSV.exists()\n",
    "test_df  = pd.read_csv(TEST_CSV) if has_test else None\n",
    "print(train_df.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9829fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3) Normalize text & brand\n",
    "def normalize_text(s):\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = s.replace(\"\\n\",\" \").replace(\"\\r\",\" \")\n",
    "    s = re.sub(r\"\\s+\",\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def extract_brand(text):\n",
    "    if not isinstance(text, str): return \"~na~\"\n",
    "    t = text.lower()\n",
    "    m = re.search(r\"item name:\\s*(.+)$\", t, flags=re.IGNORECASE|re.MULTILINE)\n",
    "    cand = text if m is None else m.group(1)\n",
    "    cand = cand.split(\" by \")[0].split(\",\")[0]\n",
    "    cand = re.sub(r\"[^a-zA-Z0-9&+\\-\\s]\", \" \", cand).strip()\n",
    "    cand = re.sub(r\"\\s+\", \" \", cand)\n",
    "    return cand[:50] if cand else \"~na~\"\n",
    "\n",
    "for df in [train_df] + ([test_df] if has_test else []):\n",
    "    df[\"catalog_content\"] = df[\"catalog_content\"].astype(str).apply(normalize_text)\n",
    "    df[\"brand\"] = df[\"catalog_content\"].apply(extract_brand)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af8ed7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4) Quantity parsing\n",
    "VOL_MAP = {\"ml\": 1.0, \"l\": 1000.0, \"fl oz\": 29.5735, \"oz\": 29.5735}\n",
    "WT_MAP  = {\"g\": 1.0, \"kg\": 1000.0, \"lb\": 453.592, \"ounce\": 28.3495}\n",
    "COUNT_TOKENS = {\"count\",\"ct\",\"pcs\",\"pieces\",\"tabs\",\"caps\",\"pack\"}\n",
    "\n",
    "_re_num   = r\"(\\d+(?:[\\.,]\\d+)?)\"\n",
    "_re_range = rf\"{_re_num}\\s*[-–]\\s*{_re_num}\"\n",
    "_re_unit  = r\"(ml|l|fl\\s*oz|ounce|oz|g|kg|lb|ct|count|pcs|pieces|tabs|caps)\"\n",
    "_re_pack  = rf\"(?:(?:pack)(?:\\s*of)?\\s*{_re_num})|(({_re_num}))\\s*pack\"\n",
    "\n",
    "def _to_float(s):\n",
    "    try: return float(str(s).replace(\",\", \".\"))\n",
    "    except: return np.nan\n",
    "\n",
    "def unit_to_base(val, unit):\n",
    "    u = unit.lower().strip()\n",
    "    if u in (\"ml\",\"l\",\"fl oz\",\"oz\"):\n",
    "        base = val if u==\"ml\" else 1000.0*val if u==\"l\" else 29.5735*val\n",
    "        return base, \"volume_ml\"\n",
    "    if u in (\"g\",\"kg\",\"lb\",\"ounce\"):\n",
    "        base = val if u==\"g\" else 1000.0*val if u==\"kg\" else 453.592*val if u==\"lb\" else 28.3495*val\n",
    "        return base, \"weight_g\"\n",
    "    if u in (\"ct\",\"count\",\"pcs\",\"pieces\",\"tabs\",\"caps\",\"pack\"):\n",
    "        return val, \"count\"\n",
    "    return np.nan, \"~none~\"\n",
    "\n",
    "def parse_total_units(text):\n",
    "    if not isinstance(text, str): return np.nan, \"~none~\"\n",
    "    t = text.lower()\n",
    "    pack_count = np.nan\n",
    "    pm = re.search(_re_pack, t)\n",
    "    if pm:\n",
    "        nums = [n for n in pm.groups() if n is not None]\n",
    "        if nums:\n",
    "            pack_count = _to_float(nums[-1])\n",
    "    m = re.search(rf\"{_re_range}\\s*{_re_unit}\", t)\n",
    "    if m:\n",
    "        a = _to_float(m.group(1)); b = _to_float(m.group(2)); u = m.group(3).strip()\n",
    "        val = np.mean([a,b])\n",
    "        amt, kind = unit_to_base(val, u)\n",
    "        if not np.isnan(amt):\n",
    "            if not np.isnan(pack_count): amt *= pack_count\n",
    "            return amt, kind\n",
    "    m = re.search(rf\"{_re_num}\\s*{_re_unit}\", t)\n",
    "    if m:\n",
    "        v = _to_float(m.group(1)); u = m.group(2).strip()\n",
    "        amt, kind = unit_to_base(v, u)\n",
    "        if not np.isnan(amt):\n",
    "            if not np.isnan(pack_count): amt *= pack_count\n",
    "            return amt, kind\n",
    "    for tok in COUNT_TOKENS:\n",
    "        m = re.search(rf\"{_re_num}\\s*{tok}\\b\", t)\n",
    "        if m:\n",
    "            cnt = _to_float(m.group(1))\n",
    "            if not np.isnan(cnt):\n",
    "                if not np.isnan(pack_count): cnt *= pack_count\n",
    "                return cnt, \"count\"\n",
    "    if not np.isnan(pack_count):\n",
    "        return pack_count, \"count\"\n",
    "    return np.nan, \"~none~\"\n",
    "\n",
    "for df in [train_df] + ([test_df] if has_test else []):\n",
    "    parsed = df[\"catalog_content\"].apply(parse_total_units)\n",
    "    df[\"total_units_base\"] = parsed.apply(lambda x: x[0])\n",
    "    df[\"unit_kind\"] = parsed.apply(lambda x: x[1])\n",
    "    df[\"is_value_pack\"] = df[\"catalog_content\"].str.contains(r\"\\b(value pack|bulk|family size)\\b\", case=False, na=False).astype(int)\n",
    "    df[\"is_refill\"]     = df[\"catalog_content\"].str.contains(r\"\\brefill\\b\", case=False, na=False).astype(int)\n",
    "    df[\"is_variety\"]    = df[\"catalog_content\"].str.contains(r\"\\bvariety\\b\", case=False, na=False).astype(int)\n",
    "    df[\"has_range\"]     = df[\"catalog_content\"].str.contains(r\"\\d+\\s*[-–]\\s*\\d+\", case=False, na=False).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5694ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5) Targets & strat labels\n",
    "def build_targets(df: pd.DataFrame):\n",
    "    tu = df[\"total_units_base\"].fillna(1.0).clip(lower=1e-6).astype(float)\n",
    "    y_price = df[\"price\"].astype(float)\n",
    "    y_unit = (y_price / tu).astype(float)\n",
    "    return y_price.values, y_unit.values, tu.values\n",
    "\n",
    "y_price, y_unit, tu_train = build_targets(train_df)\n",
    "y_unit_clip = np.clip(y_unit, np.percentile(y_unit,1), np.percentile(y_unit,99))\n",
    "scaler_y = StandardScaler().fit(np.log1p(y_unit_clip).reshape(-1,1))\n",
    "y_std = scaler_y.transform(np.log1p(y_unit).reshape(-1,1)).ravel()\n",
    "\n",
    "def make_strat_labels(y, max_bins=20, n_splits=5, min_count_per_class=5):\n",
    "    import pandas as pd, numpy as np\n",
    "    q = max_bins\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    while q >= max(3, n_splits):\n",
    "        bins = pd.qcut(np.log1p(y), q=q, duplicates=\"drop\")\n",
    "        labels = pd.Series(bins).cat.codes.to_numpy()\n",
    "        _, cnt = np.unique(labels, return_counts=True)\n",
    "        if cnt.min() >= min_count_per_class:\n",
    "            return labels\n",
    "        q -= 2\n",
    "    ranks = pd.Series(np.log1p(y)).rank().to_numpy()\n",
    "    labels = np.floor(ranks / (len(y)/float(n_splits*2))).astype(int)\n",
    "    return labels\n",
    "\n",
    "y_strat = make_strat_labels(y_unit, max_bins=20, n_splits=N_SPLITS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1071474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6) Numeric features\n",
    "def prepare_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    num = pd.DataFrame(index=df.index)\n",
    "    num[\"total_units_base\"] = df[\"total_units_base\"].fillna(df[\"total_units_base\"].median())\n",
    "    for k in [\"volume_ml\",\"weight_g\",\"count\",\"~none~\"]:\n",
    "        num[f\"unit_{k}\"] = (df[\"unit_kind\"]==k).astype(int)\n",
    "    for k in [\"is_value_pack\",\"is_refill\",\"is_variety\",\"has_range\"]:\n",
    "        num[k] = df[k].astype(int)\n",
    "    s = df[\"catalog_content\"].fillna(\"\")\n",
    "    num[\"len_chars\"] = s.str.len().clip(0, 2000)\n",
    "    num[\"len_words\"] = s.apply(lambda x: len(x.split())).clip(0, 400)\n",
    "    num[\"num_digits\"] = s.str.count(r\"\\d\").clip(0, 100)\n",
    "    return num\n",
    "\n",
    "X_num_train = prepare_numeric(train_df)\n",
    "X_num_test  = prepare_numeric(test_df) if has_test else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84deb61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7) Brand target encoding\n",
    "brands = train_df[\"brand\"].fillna(\"~na~\").astype(str).values\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "brand_te = np.zeros(len(train_df), dtype=np.float32)\n",
    "for tr, va in skf.split(np.zeros(len(y_strat)), y_strat):\n",
    "    mean_tr = y_unit[tr].mean()\n",
    "    gmean = pd.Series(y_unit[tr]).groupby(pd.Series(brands[tr])).mean()\n",
    "    enc = pd.Series(brands[va]).map(gmean).fillna(mean_tr).values\n",
    "    brand_te[va] = enc\n",
    "X_num_train[\"brand_te_per_unit\"] = brand_te\n",
    "\n",
    "topN = 500\n",
    "vc = pd.Series(brands).value_counts().head(topN).index.tolist()\n",
    "for b in vc:\n",
    "    X_num_train[f\"brand_{b}\"] = (pd.Series(brands)==b).astype(int).values\n",
    "\n",
    "if has_test:\n",
    "    brands_test = test_df[\"brand\"].fillna(\"~na~\").astype(str)\n",
    "    full_gmean = pd.Series(y_unit).groupby(pd.Series(brands)).mean()\n",
    "    test_te = brands_test.map(full_gmean).fillna(y_unit.mean()).values\n",
    "    X_num_test[\"brand_te_per_unit\"] = test_te\n",
    "    for b in vc:\n",
    "        X_num_test[f\"brand_{b}\"] = (brands_test==b).astype(int).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff65fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8) MiniLM embeddings + optional clusters\n",
    "from sentence_transformers import SentenceTransformer\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "st_model = SentenceTransformer(MODEL_NAME, device=(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "def encode_texts(texts, batch_size=256):\n",
    "    embs = st_model.encode(texts, batch_size=batch_size, show_progress_bar=True, normalize_embeddings=True)\n",
    "    return np.asarray(embs, dtype=np.float32)\n",
    "\n",
    "train_texts = train_df[\"catalog_content\"].fillna(\"\").tolist()\n",
    "minilm_train = encode_texts(train_texts)\n",
    "if has_test:\n",
    "    test_texts  = test_df[\"catalog_content\"].fillna(\"\").tolist()\n",
    "    minilm_test = encode_texts(test_texts)\n",
    "else:\n",
    "    minilm_test = None\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "K_CLUST = 100\n",
    "kmeans = MiniBatchKMeans(n_clusters=K_CLUST, random_state=RANDOM_STATE, batch_size=4096)\n",
    "clus_train = kmeans.fit_predict(minilm_train)\n",
    "clus_train_oh = np.eye(K_CLUST, dtype=np.float32)[clus_train]\n",
    "clus_test_oh = None\n",
    "if minilm_test is not None:\n",
    "    clus_test = kmeans.predict(minilm_test)\n",
    "    clus_test_oh = np.eye(K_CLUST, dtype=np.float32)[clus_test]\n",
    "\n",
    "def hstack_safe(*arrs):\n",
    "    mats = []\n",
    "    for a in arrs:\n",
    "        if a is None: continue\n",
    "        if isinstance(a, pd.DataFrame) or isinstance(a, pd.Series):\n",
    "            mats.append(a.values)\n",
    "        else:\n",
    "            mats.append(a)\n",
    "    return np.hstack(mats).astype(np.float32)\n",
    "\n",
    "X_mini_num_train = hstack_safe(minilm_train, X_num_train.values, clus_train_oh)\n",
    "X_mini_num_test  = hstack_safe(minilm_test,  X_num_test.values,  clus_test_oh) if minilm_test is not None else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7174935",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 9) TF-IDF + Ridge\n",
    "tfw = TfidfVectorizer(ngram_range=(1,2), max_features=150_000, min_df=3)\n",
    "tfc = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5), max_features=80_000, min_df=5)\n",
    "X_tfidf = sparse.hstack([tfw.fit_transform(train_df[\"catalog_content\"].fillna(\"\")),\n",
    "                         tfc.fit_transform(train_df[\"catalog_content\"].fillna(\"\"))]).tocsr()\n",
    "oof_tfidf = np.zeros(len(train_df), dtype=np.float32)\n",
    "ridge_models = []\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "for fold, (tr, va) in enumerate(skf.split(np.zeros(len(y_strat)), y_strat), 1):\n",
    "    m = Ridge(alpha=2.0, random_state=RANDOM_STATE)\n",
    "    m.fit(X_tfidf[tr], y_std[tr])\n",
    "    oof_tfidf[va] = m.predict(X_tfidf[va])\n",
    "    ridge_models.append(m)\n",
    "    print(f\"[Ridge fold {fold}] done.\")\n",
    "Xt_tfidf = None\n",
    "if has_test:\n",
    "    Xt_tfidf = sparse.hstack([tfw.transform(test_df[\"catalog_content\"].fillna(\"\")),\n",
    "                              tfc.transform(test_df[\"catalog_content\"].fillna(\"\"))]).tocsr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd7032",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 10) DNN on MiniLM+numeric\n",
    "scaler_X = StandardScaler(with_mean=True, with_std=True)\n",
    "X_mini_num_train_std = scaler_X.fit_transform(X_mini_num_train)\n",
    "X_mini_num_test_std  = scaler_X.transform(X_mini_num_test) if X_mini_num_test is not None else None\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y): self.X=torch.from_numpy(X).float(); self.y=torch.from_numpy(y).float()\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden, dropout):\n",
    "        super().__init__()\n",
    "        layers=[]; d=in_dim\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(d,h), nn.BatchNorm1d(h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            d=h\n",
    "        layers += [nn.Linear(d,1)]\n",
    "        self.net = nn.Sequential(*layers)\n",
    "    def forward(self,x): return self.net(x).squeeze(-1)\n",
    "\n",
    "def train_fold(Xtr, ytr, Xva, yva, in_dim):\n",
    "    model = MLP(in_dim, HIDDEN, DROPOUT).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    best=float(\"inf\"); patience=0; best_state=None\n",
    "    tr_dl = DataLoader(TabularDataset(Xtr,ytr), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    va_dl = DataLoader(TabularDataset(Xva,yva), batch_size=BATCH_SIZE, shuffle=False)\n",
    "    for ep in range(1,EPOCHS+1):\n",
    "        model.train(); tr_loss=0.0\n",
    "        for xb,yb in tr_dl:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            opt.zero_grad(); pred = model(xb); loss = loss_fn(pred,yb); loss.backward(); opt.step()\n",
    "            tr_loss += loss.item()*xb.size(0)\n",
    "        tr_loss/=len(tr_dl.dataset)\n",
    "        model.eval(); va_loss=0.0\n",
    "        with torch.no_grad():\n",
    "            for xb,yb in va_dl:\n",
    "                xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                pred = model(xb); loss = loss_fn(pred,yb); va_loss += loss.item()*xb.size(0)\n",
    "        va_loss/=len(va_dl.dataset)\n",
    "        if va_loss < best-1e-5: best=va_loss; patience=0; best_state={k:v.detach().cpu().clone() for k,v in model.state_dict().items()}\n",
    "        else: patience+=1\n",
    "        if ep%3==0: print(f\"Epoch {ep:02d} | train {tr_loss:.4f} | valid {va_loss:.4f} | best {best:.4f}\")\n",
    "        if patience>=PATIENCE: break\n",
    "    model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "oof_dnn = np.zeros(len(train_df), dtype=np.float32)\n",
    "dnn_models=[]\n",
    "for fold,(tr,va) in enumerate(skf.split(np.zeros(len(y_strat)), y_strat),1):\n",
    "    model = train_fold(X_mini_num_train_std[tr], y_std[tr], X_mini_num_train_std[va], y_std[va], X_mini_num_train_std.shape[1])\n",
    "    dnn_models.append(model)\n",
    "    with torch.no_grad():\n",
    "        xva = torch.from_numpy(X_mini_num_train_std[va]).float().to(DEVICE)\n",
    "        oof_dnn[va] = model(xva).cpu().numpy()\n",
    "    print(f\"[DNN fold {fold}] done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2074058",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 11) Blend OOF & evaluate\n",
    "w_dnn, w_tfidf = 0.7, 0.3\n",
    "oof_std_blend = w_dnn*oof_dnn + w_tfidf*oof_tfidf\n",
    "oof_log_per_unit = scaler_y.inverse_transform(oof_std_blend.reshape(-1,1)).ravel()\n",
    "oof_per_unit = np.expm1(oof_log_per_unit)\n",
    "oof_price = (oof_per_unit * tu_train).clip(0.01)\n",
    "cv = smape(train_df[\"price\"].values, oof_price)\n",
    "print(f\"CV SMAPE (OOF): {cv:.3f}%\")\n",
    "pd.DataFrame({\"sample_id\": train_df[\"sample_id\"], \"price_true\": train_df[\"price\"], \"price_pred\": oof_price}).to_csv(OUTPUT_DIR/\"oof_predictions_dnn.csv\", index=False)\n",
    "print(\"Saved:\", OUTPUT_DIR/\"oof_predictions_dnn.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb1045d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 12) Inference\n",
    "if has_test:\n",
    "    preds_tfidf = np.mean([m.predict(Xt_tfidf) for m in ridge_models], axis=0) if 'Xt_tfidf' in globals() and Xt_tfidf is not None else 0.0\n",
    "    # DNN preds\n",
    "    preds_dnn_folds=[]\n",
    "    with torch.no_grad():\n",
    "        xt = torch.from_numpy(X_mini_num_test_std).float().to(DEVICE)\n",
    "        for model in dnn_models:\n",
    "            preds_dnn_folds.append(model(xt).cpu().numpy())\n",
    "    preds_dnn = np.mean(preds_dnn_folds, axis=0)\n",
    "    preds_std_blend = w_dnn*preds_dnn + w_tfidf*preds_tfidf\n",
    "    log_per_unit = scaler_y.inverse_transform(preds_std_blend.reshape(-1,1)).ravel()\n",
    "    per_unit = np.expm1(log_per_unit)\n",
    "    tu_test = test_df[\"total_units_base\"].fillna(1.0).clip(lower=1e-6).astype(float).values\n",
    "    price_pred = (per_unit * tu_test).clip(0.01)\n",
    "    sub = pd.DataFrame({\"sample_id\": test_df[\"sample_id\"], \"price\": price_pred})\n",
    "    sub.to_csv(OUTPUT_DIR/\"submission_dnn.csv\", index=False)\n",
    "    print(\"Saved submission:\", OUTPUT_DIR/\"submission_dnn.csv\")\n",
    "else:\n",
    "    print(\"No test.csv detected; skipped submission.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
